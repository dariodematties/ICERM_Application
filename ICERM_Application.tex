% Source: http://tex.stackexchange.com/a/150903/23931
\documentclass{article}
\usepackage[letterpaper,margin=0.5in]{geometry}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{tgschola} % or any other font package you like
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{bibentry}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{%
  \footnotesize\sffamily
  \yourname\quad
  \jobtitle\quad
  \institution\quad}
  %web: \textcolor{blue}{\itshape\yourweb}\quad
  %\textcolor{blue}{\youremail}}

\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.3ex}{\bf +}\nolinebreak\hspace{-.10em}\raisebox{.3ex}{\bf +}}
\newcommand{\soptitle}{Dario Dematties Curriculum Vitae}
\newcommand{\yourname}{Dario Dematties Application to}
%\newcommand{\youremail}{dariodematties@yahoo.com.ar}
\newcommand{\jobtitle}{ICERM}
%\newcommand{\jobtitle}{Ph.D. candidate}
\newcommand{\institution}{Scientific Machine Learning Jan 28 - 30, 2019}
%\newcommand{\institution}{Faculty of Engineering, University of Buenos Aires}
%\newcommand{\yourweb}{https://www.abcd.com/}

\newcommand{\statement}[1]{\par\medskip
  \underline{\textcolor{blue}{\textbf{#1:}}}\space
}

\usepackage[
  colorlinks,
  breaklinks,
  pdftitle={\yourname - \soptitle},
  pdfauthor={\yourname},
  unicode
]{hyperref}


\usepackage{color}
\newcommand\redcomment[1]{\textcolor{red}{#1}}

%\usepackage{cancel}
\usepackage[normalem]{ulem}

\begin{document}


\begin{center}
%\LARGE \yourname\\
\LARGE \soptitle\\
%\large of \yourname\ (ATPESC applicant for Summer---2018)


\end{center}

\hrule
\vspace{1pt}
\hrule height 1pt

\bigskip


\statement{Education}
\begin{itemize}
	\item 2013-present {\bf Ph.D. candidate in Biomedical Engineering}\\
University of Buenos Aires, Faculty of Engineering (Argentina)
\begin{itemize}
	\item \textbf{Biologically inspired modelling} for \textbf{early language acquisition} phenomena explanation
	\item Thesis title: \textbf{Phonetic Acquisition in Cortical Dynamics, a Computational Approach}
	\item \textbf{High Performance Computing (HPC)} for science. Project running on Cooley resource at Argonne Leadership Computing Facility (ALCF) Allocation.
\end{itemize}


\item 1999--2012 {\bf BS in Electronic Engineering}\\
Universidad Tecnol\'ogica Nacional, Facultad Regional Mendoza (Argentina)
\begin{itemize}
	\item Senior Thesis: \textbf{Kalman Filter Implementation in FPGA}, Reconfigurable Computation Laboratory
\end{itemize}
\end{itemize}














\statement{Specialization Courses}
\begin{itemize}
	\item 2018 {\bf Argonne Training Program on Extreme Scale Computing (ATPESC).}\\
St. Charles, Illinois, USA\\
Organized by Argonne National Laboratory, Lemont, IL, USA\\
ATPESC provides intensive, two-week training on the key skills, approaches, and tools to design, implement, and execute computational science and engineering applications on current high-end computing systems and the leadership-class computing systems of the future.\\
Hours: 100.\\
Final Exam Approved.

	\item 2018 {\bf Computational Theories of Learning}\\
Instituto de Biología y Medicina Experimental (CONICET-IBYME), CABA, Argentina\\
Hours: 36.\\
Final Exam Approved.

	\item 2018 {\bf Adaptive Systems: Neural Networks}\\
Facultad de Ingeniería, Universidad de Buenos Aires, CABA, Argentina\\
Main book: {\it Introduction To The Theory Of Neural Computation} (Santa Fe Institute Series) 1st Edition by John A. Hertz, Anders S. Krogh and Richard G. Palmer.\\
Hours: 96.\\
Grade: 10.

	\item 2018 {\bf Detection and Estimation Theory}\\
Facultad de Ingeniería, Universidad de Buenos Aires, CABA, Argentina\\
Main book: {\it Pattern Classification and Scene Analysis} by R. O. Duda and P. E. Hart, Wiley, 1st. edition 1973, 2nd. edition 2001.\\
Hours: 90.\\
Grade: 10.

	\item 2014 {\bf Real Analysis II}\\
Facultad de Ciencias Exactas y Naturales, Universidad Nacional de Cuyo, Mendoza, Argentina\\
Main book: {\it Principles of Mathematical Analysis} by Walter Rudin\\
Hours: 128.\\
Grade: 9.

	\item 2014 {\bf Real Analysis I}\\
Facultad de Ciencias Exactas y Naturales, Universidad Nacional de Cuyo, Mendoza, Argentina\\
Main book: {\it Principles of Mathematical Analysis} by Walter Rudin\\
Hours: 128.\\
Grade: 10.
\pagebreak
	\item 2013 {\bf Experimental Psychology and Neurobiology of Language}\\
Instituto de Ciencias Humanas, Sociales y Ambientales (INCIHUSA) CCT CONICET, Mendoza, Argentina\\
Possible evolutionary origins of Language, Language acquisition, Neuroscience of Language, Aphasias, Evaluation of Language, Genetics of Language\\ 
{\it Psicolog\'ia Experimental y Neurobiolog\'ia del lenguaje}\\
Hours: 30.\\
Grade: 10.

	\item 2011 {\bf Didactic of Mathematics}\\
Universidad Tecnol\'ogica Nacional, Mendoza, Argentina\\
{\it Did\'actica de la Matem\'atica}\\
Hours: 30.

\end{itemize}
 









\statement{Professional Practices}
\begin{itemize}
	\item 2012-2013 {\bf Reconfigurable Computation Laboratory}\\
Universidad Tecnol\'ogica Nacional, Facultad Regional Mendoza (Argentina)
\begin{itemize}
	\item Kalman Filter Numeric Tests in Matlab.
	\item Resource and delay efficient matrix multiplication algorithms. Implementation in VHDL.
\end{itemize}
\end{itemize}

 




\statement{Publications}
\begin{itemize}
	\item Dario Dematties, Silvio Rizzi, George~K. Thiruvathukal, Alejandro Wainselboim,
  and Silvano Zanutto.
\newblock {\em Phonetic Acquisition in Cortical Dynamics, a Computational
  Approach}.
\newblock PLOS ONE Submitted Article waiting review process, 2018.
	\item Dario Dematties and Francisco Iglesias.
\newblock {\em Kalman Filter Implementation on FPGA}.
\newblock Universidad Tecnologica Nacional, 2012.
\end{itemize}



\statement{Skills}

\begin{itemize}

	\item {\bf Message Passing Interface (MPI) and Shared Memory Multi-Processing Application Programming Interface (OpenMP).}\\
As a Ph.D. student, \textbf{MPI+OpenMP} hybrid implementations running on \textbf{Cooley cluster} at \textbf{Argonne Leadership Computing Facility}.
Strong and weak scaling tests with simulations of up to \textbf{64 nodes (one MPI rank per node)} with
\textbf{12 threads} running in \textbf{each node (OpenMP)}.
	
	\item {\bf General-Purpose Programming Languages.}
	\begin{itemize}
		\item \textbf{C}, \textbf{\CC11} and \textbf{\CC14} \textbf{Standard Template Libraries}.
		\item \textbf{Python} Interpreted high-level programming language.
		\item \textbf{MPI for Python} Distributed Memory Parallel Audio Files Generation by means of Speech Synthesizer.
		\item \textbf{Object Oriented Programming}: Inheritance, composition and polymorphism.
	\end{itemize}

	\item \textbf{Interpreted numerical computing languages}
\textbf{GNU Octave} and \textbf{Matlab}.
In fact, I know the internal file format specification of those languages
since I had to implement my own libraries for my Ph.D. project.

	\item {\bf Natural Language Processing (NLP).}\\
n-gram models for words frequency estimation
under maximum-likelihood, Laplace, Lidstone, Jeffreys-Perks
and Good-Turing hypotheses.
Testing by means of held-out estimation and cross validation.

	\item {\bf Detection and Estimation Theory.}\\
non-parametric algorithms for the estimation of random distributions
by means of Parzen, k-Nearest Neighborhood
and The Nearest Neighbor algorithms.
Dirichlet Distribution and Dirichlet Process
(Clustering of a mixture of Gaussians).
Suppor Vector Machine classification.

	\item {\bf Artificial Neural Networks.}\\
Hopfield Networks, simple and multilayer Perceptrons (Backpropagation).
Self Organizing Maps unsupervised clustering. Growing Neural Gas unsupervised clustering.

	\item {\bf Version control systems and Web-based Git repository.}\\
\textbf{GIT} and \textbf{Apache Subversion (svn)}.
\textbf{GitLab} and \textbf{GitHub}.

	\item {\bf Debugging and Profiling.}\\
\textbf{GNU (GDB) and Arm DDT Debuggers}.\\
For \textbf{profiling} purposes,
\textbf{ARM Forge Map}, \textbf{Valgrind} and \textbf{Kcachegrind}.

	\item {\bf Hardware Description Language.}\\
In the world of \textbf{Hardware Description Language (VHDL)},
I implemented an instance of the \textbf{Kalman Filter in a FPGA device}
for my Bachelor Degree thesis.
	
	\item {\bf Inter-process and Network communication socket.}\\
Unix domain socket \textbf{(POSIX socket)} and \textbf{POSIX threads (pthread)} with the implementation of a HTTP server as
a project for a subject at university.

	\item {\bf PIC Microcontrolers, Symbolic Computation, Unix Like Environment, Document Preparation System.}
	\begin{itemize}
		\item Assembler for \textbf{PIC microcontrollers}
		\item Symbolic Computation in \textbf{wxMaxima} and \textbf{Wolfram Mathematica}.
		\item I feel comfortable in the \textbf{Unix like environment}. In fact, I use and administrate
		my Desktop and Laptop with Linux and now I am using \textbf{RHEL distribution} on
		\textbf{Cooley nodes at Argonne}.
		\item \LaTeX, Beamer.
		\item I also use \textbf{Vim} as my \textbf{default text editor program}
		not only because it is efficient, but also because it allows me to devote my complete
		attention to the algorithmic implementation putting the text edition
		in background.
	\end{itemize}
	
\end{itemize}

\statement{Awards}
\begin{itemize}
	\item 2013 \textbf{Special Mention in \textit{Concurso de Preingenier\'ia 2013} (Argentina)}.\\
With the work entitled: Kalman Filter Implementation in FPGA.
{\it Implementaci\'on de filtro de Kalman en FPGA}
\end{itemize}
 




\statement{Scholarships}

\begin{itemize}
	\item 2013 \textbf{UBACYT 2013}. Ph.D. Funding. Universidad de Buenos Aires. (Argentina)
	\item 2012 \textbf{Research Scholarship}. Electronic Instrumentation.
		Nanotechnology Laboratory at Universidad Tecnol\'ogica Nacional Facultad Regional Mendoza. (Argentina)
\end{itemize}
 










\statement{Participation in congresses, conferences and symposia}
\begin{itemize}
	\item 2018 {\bf XVI Argentinian Society of Linguistic Studies} \emph{Sociedad Argentina de Estudios Lingüísticos (SAEL)}\\
{Escuela de Humanidades de la Universidad Nacional de San Mart\'in, Buenos Aires, Argentina}\\
Presentation.\\
\textbf{Conference Speaker}.\\
Title: A biologically Plausible Computational Model for Phonetic Classification\\
{\it The role of the predictability during language processing}\\
Year: 2018.\\
	\item 2015 {\bf IV Biomedical Engineering Days}\\
Facultad de Ingenier\'ia de la Universidad de Buenos Aires, Buenos Aires, Argentina\\
{\it IV Jornadas de Ingenier\'ia Biom\'edica}\\
\textbf{Symposia, Poster Presentation}.\\
\textbf{Conference Speaker}.\\
Title: Neurocomputational Theories for the Study of Grammar Acquisition from the Learning of Categories\\
{\it Teor\'ias Neurocomputacionales para el Estudio de la Adquisici\'on de Gram\'atica a partir del Aprendizaje de Categor\'ias}\\
Year: 2015.\\
\pagebreak
	\item 2015 {\bf Neurocog 2015}\\
Facultad de Ciencias Exactas y Naturales de la Universidad de Buenos Aires, Buenos Aires, Argentina\\
Title: Development of a Computational Neural Network Model for Language Acquisition\\
{\it Desarrollo de un modelo computacional de redes neuronales para la adquisici\'on del lenguaje}\\
\textbf{Symposia, Poster Presentation}.\\
Year: 2015.\\

\end{itemize}
 







\statement{Graduate Teaching Experience}
\begin{itemize}
	\item 2016 {\bf Associate Professor}\\
Linguistics and Experimental Neurobiology of Language INCIHUSA, CCT Mendoza, Argentina\\
Exposed topics: \textbf{Hierarchical Temporal Memory, Cortical Learning Algorithm, Sparse Distributed Representations, Semantic Folding Theory}.\\
Tittle: From infant brain to adult brain: cognition development based on concepts from a cognition based on details.
{\it Del cerebro infantil al cerebro adulto: desarrollo de la cognici\'on basada en conceptos desde una cognici\'on basada en detalles}.
	\item 2015 {\bf Associate Professor}\\
Linguistics and Experimental Neurobiology of Language INCIHUSA, CCT Mendoza, Argentina\\
Exposed topics: \textbf{Memory Prediction Framework, Hierarchical Temporal Memory,
Modeling children's early grammatical knowledge with Bayesian Processes}.\\
Tittle: Neurobiological and Computational Foundations of Cognition and Language.
{\it Fundamentos neurobiol\'ogicos y computacionales de la cognici\'on y el lenguaje}.
\end{itemize}
 





\statement{Undergraduate Teaching Experience}
\begin{itemize}
	\item 2013-2013 {\bf University teacher/assistant in charge of assignments}\\
Universidad Ju\'an Agust\'in Maza, Mendoza, Argentina\\
Subject: \textbf{Mathematical Analysis II}.

	\item 2013-2013 {\bf University First Assistant}\\
Universidad Tecnol\'ogica Nacional, Facultad Reginoal Mendoza, Argentina\\
Subject: \textbf{Signals and Systems Analysis}.

	\item 2013-2013 {\bf University First Assistant}\\
Universidad Tecnol\'ogica Nacional, Facultad Reginoal Mendoza, Argentina\\
Subject: \textbf{Mathematical Analysis II}.

	\item 2012-2013 {\bf University Second Assistant}\\
Universidad Tecnol\'ogica Nacional, Facultad Reginoal Mendoza, Argentina\\
Subject: \textbf{Signals and Systems Analysis}.

	\item 2012-2013 {\bf University Tutor}\\
Universidad Tecnol\'ogica Nacional, Facultad Reginoal Mendoza, Argentina\\
Subject: \textbf{Signals and Systems Analysis}.

	\item 2011-2013 {\bf University Second Assistant.}\\
Universidad Tecnol\'ogica Nacional, Facultad Reginoal Mendoza, Argentina\\
Subject: \textbf{Mathematical Analysis II}.

	\item 2007-2013 {\bf University Tutor.}\\
Universidad Tecnol\'ogica Nacional, Facultad Reginoal Mendoza, Argentina\\
Subject: \textbf{Mathematical Analysis II}.

\end{itemize}
 
\newpage


%\section*{Personal Statement}
%\subsection*{Research Interests}

%As a Ph.D. student at the Institute of Biomedical Engineering, I try to identify neuroanatomical and physiological features in mammalian cortical tissue that could leverage phonetic perception invariance and generalization in biologically inspired computational models. My computational approach is completely unsupervised and incorporates key nerophysiological and anatomical properties that exist in brain cortex. I run my model on Cooley nodes (a visualization and analysis cluster at Argonne National Laboratory). During the last months my main activity has been related to High Performance Computing and to run profiling test of my computational approach on Cooley (Argonne Leadership Computing Facility (ALCF) Allocation). As a derivation of that, I was selected from a pool of more than a hundred applicants after a review process to attend the Argonne Training Program on Extreme Scale Computing (ATPESC) which was held in St. Charles, IL, USA. In such program I received key skills, approaches, and tools necessary to design, implement, and execute Computational Science and Engineering applications on current high-end systems and the Leadership-Class systems expected to be available in 2019 and beyond. As a result, I have been able to implement my biologically plausible computational approach on Cooley nodes focussing on an eventual migration to a leadership supercomputer as Theta at Argonne.

%The model our group elaborated is called Cortical Spectro-Temporal Model (CSTM) and incorporates cortical neurophysilogical features, such as Columnar Organization and Sparse Distributed Representations (SDRs)--among others. SDRs have extraordinary mathematical properties which give them high noise rejection and fault tolerance. Those are typical characteristics in cortical tissue where individual cells are far from 100\% reliable and they die and regenerate continuously. According to recent findings in neuroscience, the brain uses SDRs to process information. This is true for all mammals, from mice to humans.

%The CSTM is completely unsupervised and returns phonetic features that improve the classification accuracy levels of the Support Vector Machine (SVM) algorithm in word classification tasks. This improvement is achieved in the presence of certain disturbances such as white noise and reverberation and under pitch variation conditions.

%The algorithms in the CSTM are written in C++14. They consist of a set of classes interrelated by inheritance and composition. The classes are parallelized by means of MPI+OpenMP. This implementation stores its outputs in Matlab and Octave file formats and uses MPI parallel I/O file systems.

%We tested our MPI+OpenMP hybrid implementation on Cooley nodes and scaled the model on up to 64 nodes with 8 OpenMP threads each, running models of up to 16384 cortical columns (a total of 3686400 neural units and 1706803200 synapses).

%Our group submitted a manuscript to Plos One with invariant phonetic word classification science results computed on Cooley. This is still under peer review process. We are writing a second manuscript that shows computational profiling performance observing how our model scales in the nodes of Cooley in terms of its parallel efficiency.


%\subsection*{Research Plans}

%My next goal is to run this project on a leadership class machine such as Theta at ALCF. With such resources, our group will be able to test model instances which differ by orders of magnitude with respect to the models we are able to test now. Emergent properties could arise from instances in which not only the number of neurons, but also its dimensionality at columnar and cortical level will be modified. Those changes could result in unforeseen behaviours in terms of phonetic feature abstraction and invariance. My participation in ATPESC 2018 gave me the needed skills in order to take the best advantage from such resources.

%I am planning to apply reinforcement learning techniques such as Temporal Difference (TD) learning in order to leverage the word classification invariance and generalization performance of our model.

%Further in the future, as a postdoctoral project, I am planning to investigate the use of Biologically-inspired natural language understanding techniques such as Semantic Folding Theory by Cortical.io (Francisco De Sousa Webber) (https://www.cortical.io/) which uses Sparse Distributed Representations (SDRs) as semantic fingerprints. I am interested in incorporating grammatical knowledge to such representations using knowledge representations such as Head Driven Phrase Structure Grammar. In fact, our computational approach uses SDRs in order to process information and introduces sequence learning capabilities with a structure of cortical columnar organization which could be tested for semantic clustering in future applications.

%\subsection*{Reasons for Wishing to Participate}
%The current machine learning (ML) revolution inflicts a highly dynamical landscape which imposes a continual updating from a scientific community that is even beyond ML and AI in itself. On the other hand, neuroscience community has made an outstanding progress in the last decades providing us with a level of knowledge about the brain that nobody had before. Nevertheless, ML and AI mainstreams seem to ignore great part of that knowledge. This could be attributed in part to the success accomplished by some AI approaches–such as Deep Convolutional Neural Networks–which while ignoring biology have achieved classification accuracy levels without precedent in the last years. Beyond that, some researchers from AI community support the idea that in order to overcome current AI limitations and to get really intelligent machines it will be necessary to observe more the brain. I endorse such idea identifying neuroanatomical and physiological cortical mammalian features that could leverage phonetic perception invariance and generalization in biologically inspired computational models. As a result of that, I am really keen on learning the most advanced features in more biologically inspired computational models that could leverage current deep learning techniques.

%Human language is a characteristic that strongly distinguish our specie from others, specially in regards to highly abstract semantic reasoning. I am eager to learn more about the lattest advancements in Text Embeddings techniques such as Word2vec by Google (Mikolov) and GloVe by Stanford (Pennington, Socher and Manning) to generate semantic clustering and to analyze more biologically inspired implementations of those.

%Participating in this workshop will undoubtedly contribute to my background in order to be in ideal conditions to complete experiments and publish results of my PhD dissertation. My next step will be a Postdoctoral position to further my research in Biologically Inspired ML, AI and Neuromorphic processing. I am truly interested in pursuing a career in Academia and to apply for a position as an Assistant Researcher at the National Council of Science and Technology (CONICET) in Argentina. Attending Scientific Machine Learning at the ICERM will considerably increase my chances of success in these early stages of my career.

%\subsection*{Additional Remarks}
%I have a genuine interest in neuromorphic processors. My Ph.D. project is intimately related to finding neurophysiological features in cortical tissue which show to be relevant for information processing and, in such case, could be relevant for future neuromorphic processor designs.

%All the information about our project is openly available on a GitLab repository at Argonne National Laboratory: https://xgitlab.cels.anl.gov/srizzi/neurophon/.














\section*{Personal Statement}

As a Ph.D. student at the Institute of Biomedical Engineering, I try to identify neuroanatomical and physiological
features in mammalian cortical tissue that could leverage phonetic perception invariance and generalization in
biologically inspired computational models. I run my model on Cooley nodes (a visualization and analysis cluster
at Argonne National Laboratory). During the last months my main activity has been related to High Performance
Computing and to run profiling test of my computational approach on Cooley.
In such context, I was selected from a pool of more than a hundred
applicants after a review process to attend the Argonne Training Program on Extreme Scale Computing (ATPESC)
which was held in St. Charles, IL, USA. I received key skills, approaches, and tools necessary to design,
implement, and execute Computational Science and Engineering applications on current high-end systems and
the Leadership-Class systems expected to be available in 2019 and beyond. As a result, I was able to implement
my biologically plausible computational approach on Cooley nodes focusing on an eventual migration to a leadership
supercomputer as Theta at Argonne.

My next goal is to run this project on a leadership class machine such as Theta at ALCF. With such resources, our
group will be able to test model instances which differ by orders of magnitude with respect to the models we are able
to test now. Emergent properties could arise from such instances.

Participating in this workshop will undoubtedly contribute to my background in order to be in ideal conditions to
complete experiments and publish results of my PhD dissertation. My next step will be a Postdoctoral position to
further my research in Biologically Inspired ML, AI and Neuromorphic processing.
Attending Scientific Machine Learning at the ICERM will considerably increase
my chances of success in these early stages of my career.









\section*{Poster Title}

Phonetic acquisition in cortical dynamics, a computational approach 

\section*{Poster Abstract}

Striking computational properties emerge from certain features of the mammalian
cortex in general. Understanding how phonetic invariance and generalization can arise
from such features is an outstanding challenge in neuroscience. Furthermore,
overcoming this challenge could shed light on the mechanisms governing other
modalities, given the anatomical and physiological uniformity found in cortical tissue.
We base our work on a set of cortical features in the mammalian cortex which we
believe could be relevant for invariance and generalization in phonetic perception. We
incorporate such features in a computational model that shows to explain invariance
and generalization in the classification of words with different number of syllables. The
outcome is a computational model, inspired by the biology of cortical tissue, which can
learn and robustly recognize the phonetic structure of words in the presence of different
disturbances–not present during training–affecting the auditory stimulus. This research
has significant potential to open up new–and more biologically accurate–alternatives to
overcome limitations in current deep feature extraction methods.














\subsection*{Additional Remarks}

Our group submitted a manuscript to Plos One with invariant phonetic word classification science results computed
on Cooley. This is still under peer review process. We are writing a second manuscript that shows computational
profiling performance observing how our model scales in the nodes of Cooley in terms of its parallel efficiency.

I am really keen on learning the most advanced
features in more biologically inspired computational models that could leverage current deep learning techniques.
Human language is a characteristic that strongly distinguish our specie from others, specially in regards to highly
abstract semantic reasoning. I am eager to learn more about the lattest advancements in Text Embeddings techniques
such as Word2vec by Google (Mikolov) and GloVe by Stanford (Pennington, Socher and Manning) to generate semantic
clustering and to analyze more biologically inspired implementations of those.

I am planning to investigate the use of Biologically-inspired natu-
ral language understanding techniques such as Semantic Folding Theory by Cortical.io (Francisco De Sousa Webber)
(https://www.cortical.io/) which uses Sparse Distributed Representations (SDRs) as semantic fingerprints. I am inter-
ested in incorporating grammatical knowledge to such representations using knowledge representations such as Head
Driven Phrase Structure Grammar.

I have a genuine interest in neuromorphic processors. My Ph.D. project is intimately related to finding neurophysiolog-
ical features in cortical tissue which show to be relevant for information processing and, in such case, could be relevant
for future neuromorphic processor designs.

All the information about our project is openly available on a GitLab repository at Argonne National Laboratory:
https://xgitlab.cels.anl.gov/srizzi/neurophon/.

Computational Implementation Co-advisors: Silvio Rizzi and George K. Thiruvathukal




\section*{Silvanos Letter}

To Whom It May Concern,

This letter is to recommend Mr. Dematties to the Scientific Machine Learning workshop to be held at the Institute for Computational and Experimental Research in Mathematics (ICERN). I am Mr. Dematties’ Ph.D. advisor. I have a position as a full Professor and Director of the Department of Biomedical Engineering (IIBM) at the University of Buenos Aires. I am also a researcher of the National Council of Science and Technology (CONICET) . I am a pioneer and a leader in the field of computational biology, neural network modeling and its applications to system neuroscience and behavior. I am the Principal Investigator in several grants from the Argentinean National Science Foundation (MINCyT) and CONICET. I have published in world class journals like PloS Computational Biology, PLoS ONE, Behavioral Neuroscience, Neural Networks, Neuroscience, The Journal of Neurochemistry and Neuroscience, among others. Additionally, I have served as a reviewer for multiple journals and governmental agencies around the world. For all these reasons, I am in a good position to describe Mr. Dematties’ research and qualifications.
Mr. Dematties’ Ph.D. thesis is based on the construction of computational models which gather relevant neurophysiological principles for information processing in the brain, regarding to language cognitive tasks. Strictly speaking, his research addresses the automatic abstraction of phonetic characteristics which are immersed in the phonotactic rules of a particular language. Unlike other perceptual automatic feature acquisition technologies, which provide with invariance to diverse supervised classification techniques -like Deep Learning- this new approach uses recently discovered neurophysiological properties. Such properties have yet to be used in algorithms of similar characteristics.
As a Ph.D. candidate, Mr. Dematties has undertaken this project with total conviction, enthusiasm and commitment. First, he implemented his hypotheses using Object Oriented Paradigm by means of C++ 14 compliant Standard Template Library (STL) classes. Second, he parallelized his code in order to use multiple CPUs in a shared memory system by means of OpenMP. Finally, he migrated the two uppermost classes in the inheritance hierarchy in order to run in distributed memory systems with MPI.  His computational model incorporates cortical neurophysilogical features, such as Columnar Organization and Sparse Distributed Representations (SDRs)--among others. SDRs have extraordinary mathematical properties which give them high noise rejection and fault tolerance. Those are typical characteristics in cortical tissue where individual cells are far from 100\% reliable and they die and regenerate continuously. According to recent findings in neuroscience, the brain uses SDRs to process information. This is true for all mammals, from mice to humans.
To the best of my knowledge, there are no precedents of computational approaches like this in large-scale resources. There are enormous potential and flexibility in this design, which allows Mr. Dematties not only to change the number of neural units in the models, but mainly the dimensionality in the array of neural units and cortical column objects with complete automatic reconfiguration of its connection scheme. With such changes, the inhibitory interactions among clusters of neural units will change dramatically. For example, with a four-dimensional array of neural units, we will be able to simulate cortical columns of about 34000 cells\footnote{The number of neurons per cortical column -in rats- varies between 10,000 and 30,000 within individual animals.	http://www.kurzweilai.net/neuroscientists-find-cortical-columns-in-brain-not-uniform-challenging-large-scale-simulation-models}. We can have thousands of cortical columns organized in multidimensional arrays, too. In a leadership supercomputer such as Theta at Argonne National Laboratory, we could be running 527 neural units per thread in a CPU. Furthermore, if we configure a three-dimensional array with 1000 cortical columns per cortical layer, a model of 4 layers could be running on about 256000 threads. Such simulations would allow us to perform experiments with substantial biological plausibility from the point of view of the neural resources in cortical tissue and with a considerable amount of cortical columns, as well.
While Mr. Dematties’s model have shown good phonetic abstraction performance on invariant word classification tasks, future experiments with more biological plausibility, with larger models running in leadership supercomputers could mean different challenges in terms of technical implementation and ML base knowledge. Therefore, I am convinced Mr. Dematties needs highly advanced knowledge interchange in cutting edge ML and AI strategies as well as a strong background in the last advancements on biologically inspired ML and General AI in order to obtain outstanding science results in the future steps of his career.
To conclude, and for all the above reasons, I do strongly recommend Mr. Dematties for the ICERN Scientific Machine Learning Workshop. Please do not hesitate to reach out to me if you have any additional questions.

Sincerely,



B. SilvanoZanutto, PhD
Full Professor- University of Buenos Aires
IBYME-CONICET, Argentina
email: silvano@fi.uba.ar


\end{document}
